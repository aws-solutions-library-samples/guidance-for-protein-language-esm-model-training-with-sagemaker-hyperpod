apiVersion: v1
kind: Service
metadata:
  name: etcd
spec:
  ports:
    - name: etcd-client-port
      port: 2379
      protocol: TCP
      targetPort: 2379
  selector:
    app: etcd

---
apiVersion: apps/v1
kind: Deployment
metadata:
  labels:
    app: etcd
  name: etcd
spec:
  replicas: 1
  selector:
    matchLabels:
      app: etcd
  template:
    metadata:
      labels:
        app: etcd
    spec:
      containers:
        - name: etcd
          command: ["/usr/local/bin/etcd"]
          args:
            - "--data-dir"
            - "/var/lib/etcd"
            - "--enable-v2"
            - "--listen-client-urls"
            - "http://0.0.0.0:2379"
            - "--advertise-client-urls"
            - "http://0.0.0.0:2379"
            - "--initial-cluster-state"
            - "new"
          image: quay.io/coreos/etcd:v3.5.19
          ports:
            - containerPort: 2379
              name: client
              protocol: TCP
            - containerPort: 2380
              name: server
              protocol: TCP
      restartPolicy: Always
---
apiVersion: "kubeflow.org/v1"
kind: PyTorchJob
metadata:
  name: bionemo-esm2
spec:
  elasticPolicy:
    rdzvBackend: etcd
    rdzvHost: etcd
    rdzvPort: 2379
    minReplicas: 1
    maxReplicas: 64
    maxRestarts: 100
    metrics:
      - type: Resource
        resource:
          name: cpu
          target:
            type: Utilization
            averageUtilization: 90
  pytorchReplicaSpecs:
    Worker:
      replicas: 2
      template:
        metadata:
          annotations:
            sidecar.istio.io/inject: "false"
        spec:
          tolerations:
            - key: nvidia.com/gpu
              operator: Exists
              effect: NoSchedule
          volumes:
          - name: fsx-pv-storage
            persistentVolumeClaim:
              claimName: fsx-claim
          containers:
            - name: bionemo
              image: ${REGISTRY}${DOCKER_IMAGE_NAME}${TAG}
              resources:
                requests:
                  nvidia.com/gpu: $GPU_PER_NODE
                  vpc.amazonaws.com/efa: $EFA_PER_NODE
                limits:
                  nvidia.com/gpu: $GPU_PER_NODE
                  vpc.amazonaws.com/efa: $EFA_PER_NODE
              env:
                - name: NCCL_DEBUG
                  value: "INFO"
              volumeMounts:
                - mountPath: /fsx-shared
                  name: fsx-pv-storage
              imagePullPolicy: Always
              command:
                - "python3"
                - /workspace/bionemo2/sub-packages/bionemo-esm2/src/bionemo/esm2/scripts/train_esm2.py
                - --train-cluster-path ${DATA_DIR}/2024_03_sanity/train_clusters_sanity.parquet \
                - --train-database-path ${DATA_DIR}/2024_03_sanity/train_sanity.db \
                - --valid-cluster-path ${DATA_DIR}/2024_03_sanity/valid_clusters.parquet \
                - --valid-database-path ${DATA_DIR}/2024_03_sanity/validation.db \
                - --precision="bf16-mixed" \
                - --num-gpus ${GPU_PER_NODE} \
                - --num-nodes 2 \
                - --num-steps 100 \
                - --val-check-interval 25 \
                - --max-seq-length 1024 \
                - --limit-val-batches 2 \
                - --micro-batch-size 2 \
                - --num-layers 33 \
                - --hidden-size 1280 \
                - --num-attention-head 20 \
                - --ffn-hidden-size 5120 \
                - --tensor-model-parallel-size 1 \
                - --create-tensorboard-logger \
                - --result-dir ${DATA_DIR}